{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25cb3378",
   "metadata": {},
   "source": [
    "## Normalization in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761d16cc",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb29083",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5d496b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "285ba166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20783eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['change','changing','changes','changed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf39cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7078d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af516a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chang'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.stem('changing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b908d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change --> chang\n",
      "changing --> chang\n",
      "changes --> chang\n",
      "changed --> chang\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word,\"-->\", p.stem(word) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9826849",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = 'The constant flux of life necessitates embracing change, whether its adapting to the changes around us or actively changing ourselves to meet new challenges.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b25629ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The constant flux of life necessitates embracing change, whether its adapting to the changes around us or actively changing ourselves to meet new challenges.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ca04844",
   "metadata": {},
   "outputs": [],
   "source": [
    "## from nltk.tokenize import word_tokenize  ## this is creating problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23244689",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenss= word_tokenize(sen)   ## this is creating problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff1a6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbbea04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5db9444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token=word_tokenizer.tokenize(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "037d1c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'constant',\n",
       " 'flux',\n",
       " 'of',\n",
       " 'life',\n",
       " 'necessitates',\n",
       " 'embracing',\n",
       " 'change',\n",
       " ',',\n",
       " 'whether',\n",
       " 'its',\n",
       " 'adapting',\n",
       " 'to',\n",
       " 'the',\n",
       " 'changes',\n",
       " 'around',\n",
       " 'us',\n",
       " 'or',\n",
       " 'actively',\n",
       " 'changing',\n",
       " 'ourselves',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'new',\n",
       " 'challenges',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a174134f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "constant\n",
      "flux\n",
      "of\n",
      "life\n",
      "necessit\n",
      "embrac\n",
      "chang\n",
      ",\n",
      "whether\n",
      "it\n",
      "adapt\n",
      "to\n",
      "the\n",
      "chang\n",
      "around\n",
      "us\n",
      "or\n",
      "activ\n",
      "chang\n",
      "ourselv\n",
      "to\n",
      "meet\n",
      "new\n",
      "challeng\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for word in token:\n",
    "    print(p.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47acdfe",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0690e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "\n",
    "# # Download WordNet explicitly to your NLTK data folder\n",
    "# nltk.download('wordnet', download_dir=r\"D:\\AiQuest\\Class_13_Text Data Processing\\Text Data Processing and Vectorizer\\nltk_data\")\n",
    "# nltk.download('omw-1.4', download_dir=r\"D:\\AiQuest\\Class_13_Text Data Processing\\Text Data Processing and Vectorizer\\nltk_data\")\n",
    "\n",
    "# # Tell NLTK to use that folder\n",
    "# nltk.data.path.append(r\"D:\\AiQuest\\Class_13_Text Data Processing\\Text Data Processing and Vectorizer\\nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89ff290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "131da395",
   "metadata": {},
   "outputs": [],
   "source": [
    "le= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df3f0949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f2bea5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "constant\n",
      "flux\n",
      "of\n",
      "life\n",
      "necessitates\n",
      "embracing\n",
      "change\n",
      ",\n",
      "whether\n",
      "it\n",
      "adapting\n",
      "to\n",
      "the\n",
      "change\n",
      "around\n",
      "u\n",
      "or\n",
      "actively\n",
      "changing\n",
      "ourselves\n",
      "to\n",
      "meet\n",
      "new\n",
      "challenge\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for w in token:\n",
    "    print(le.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eedd6582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change --> change\n",
      "changing --> changing\n",
      "changes --> change\n",
      "changed --> changed\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(w,\"-->\",le.lemmatize(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c63fc82",
   "metadata": {},
   "source": [
    "## Tokenization in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea3bec2",
   "metadata": {},
   "source": [
    "NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca18b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "## from nltk.tokenize import word_tokenize,sent_tokenize  ## not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31db597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer,PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50aac6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer=TreebankWordTokenizer()\n",
    "sen_tokenizer=PunktSentenceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b210e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'from', 'aiQuest', 'Intelligence.', 'I', 'am', 'learning', 'NLP.', 'It', 'is', 'fascinating', '!']\n",
      "[\"I'm from aiQuest Intelligence.\", 'I am learning NLP.', 'It is fascinating!']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I'm from aiQuest Intelligence. I am learning NLP. It is fascinating!\"\n",
    "\n",
    "word_tkn= word_tokenizer.tokenize(sentence)\n",
    "sent_tkn= sen_tokenizer.tokenize(sentence)\n",
    "\n",
    "print(word_tkn)\n",
    "print(sent_tkn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b613129a",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c0cce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d71060c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1651b564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I'm from aiQuest Intelligence. I am learning NLP. It is fascinating!"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spc = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"I'm from aiQuest Intelligence. I am learning NLP. It is fascinating!\"\n",
    "doc= spc(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7106deed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bc3aa91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'from', 'aiQuest', 'Intelligence', '.', 'I', 'am', 'learning', 'NLP', '.', 'It', 'is', 'fascinating', '!']\n"
     ]
    }
   ],
   "source": [
    "word_tokens=[token.text for token in doc ]\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d7478f",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2333cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a93e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6bd34d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e063290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I'm from aiQuest Intelligence. I am learning NLP. It is fascinating!\"\n",
    "tokens=tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61217924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'from',\n",
       " 'ai',\n",
       " '##quest',\n",
       " 'intelligence',\n",
       " '.',\n",
       " 'i',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'nl',\n",
       " '##p',\n",
       " '.',\n",
       " 'it',\n",
       " 'is',\n",
       " 'fascinating',\n",
       " '!']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f080e",
   "metadata": {},
   "source": [
    "## Named Entity Tokenization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b29314b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nltk.download('wordnet')\n",
    "## nltk.download('maxent_ne_chunker') \n",
    "## nltk.download('words') \n",
    "## nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce59a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TreebankWordTokenizer,pos_tag,ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f74ab4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I'm from aiQuest Intelligence. I am learning NLP. It is fascinating!, Hasan Khan, my name is Joe\"\n",
    "\n",
    "tokens=TreebankWordTokenizer()\n",
    "tokenize=tokens.tokenize(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12306451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'m\",\n",
       " 'from',\n",
       " 'aiQuest',\n",
       " 'Intelligence.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'NLP.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'fascinating',\n",
       " '!',\n",
       " ',',\n",
       " 'Hasan',\n",
       " 'Khan',\n",
       " ',',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Joe']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d0707f",
   "metadata": {},
   "source": [
    "pos_tag using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62afa48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spc = spacy.load('en_core_web_sm')\n",
    "\n",
    "## text = \"I'm from aiQuest Intelligence. I am learning NLP. It is fascinating!\"\n",
    "text = \"Shakil Khan lives in Germany Europe\"\n",
    "tokenize2=spc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cdb19ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Shakil Khan lives in Germany Europe"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e3b6c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens=[(i.text,i.pos_) for i in tokenize2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aee8f2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Shakil', 'PROPN'),\n",
       " ('Khan', 'PROPN'),\n",
       " ('lives', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('Germany', 'PROPN'),\n",
       " ('Europe', 'PROPN')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b004277a",
   "metadata": {},
   "source": [
    "doc.ents\n",
    "\n",
    "When you pass text to spaCy (doc = nlp(text)), spaCy automatically finds named entities.\n",
    "\n",
    "doc.ents is a list of all detected entities.\n",
    "\n",
    "Entities are things like:\n",
    "\n",
    "PERSON (people)\n",
    "\n",
    "ORG (organizations)\n",
    "\n",
    "GPE (cities, countries)\n",
    "\n",
    "DATE (dates)\n",
    "\n",
    "MONEY\n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a6a7e567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shakil Khan PERSON\n",
      "Germany GPE\n",
      "Europe LOC\n"
     ]
    }
   ],
   "source": [
    "for ent in tokenize2.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d69721",
   "metadata": {},
   "source": [
    "## Text Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "32940d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da83e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "040eee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46981364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey, I love Bangladesh;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good afternoon, I am happy!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I live in Germany</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice to meet you man-</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You won an iPhone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text  class\n",
       "0      Hey, I love Bangladesh;      1\n",
       "1  Good afternoon, I am happy!      1\n",
       "2            I live in Germany      1\n",
       "3        Nice to meet you man-      1\n",
       "4            You won an iPhone      0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f263fcc9",
   "metadata": {},
   "source": [
    "Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11b7888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9047fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import nltk\n",
    "## nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8a1a75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fa86778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ae8b3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['albanian',\n",
       " 'arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'belarusian',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'tamil',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a83db6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "df80e335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2b3ea0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text): \n",
    "    \n",
    "    remove_punc = [char for char in text if char not in string.punctuation] # Remove punctuation\n",
    "    clean_words = ''.join(remove_punc) # char joining\n",
    "    split_words = clean_words.split()\n",
    "\n",
    "    #Remove stopwords\n",
    "    text= [word for word in split_words if word.lower() not in en_words]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1a551080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(preprocess_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "86680ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [Hey, love, Bangladesh]\n",
       "1    [Good, afternoon, happy]\n",
       "2             [live, Germany]\n",
       "3           [Nice, meet, man]\n",
       "4                    [iPhone]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "36538d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0b7458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    lemmatize_text=' '.join([lemmatized.lemmatize(word) for word in text])\n",
    "    return lemmatize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "389c34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "805b6e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey love Bangladesh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good afternoon happy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>live Germany</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice meet man</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iPhone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   text  class\n",
       "0   Hey love Bangladesh      1\n",
       "1  Good afternoon happy      1\n",
       "2          live Germany      1\n",
       "3         Nice meet man      1\n",
       "4                iPhone      0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf022a52",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ba60e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e13e3d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c11d0f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_x = cv.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ccd6ef04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 12 stored elements and shape (5, 12)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5f9c3225",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names=cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "acae26cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['afternoon', 'bangladesh', 'germany', 'good', 'happy', 'hey',\n",
       "       'iphone', 'live', 'love', 'man', 'meet', 'nice'], dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0f259706",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df=pd.DataFrame(cv_x.toarray(),index=df['text'],columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>afternoon</th>\n",
       "      <th>bangladesh</th>\n",
       "      <th>germany</th>\n",
       "      <th>good</th>\n",
       "      <th>happy</th>\n",
       "      <th>hey</th>\n",
       "      <th>iphone</th>\n",
       "      <th>live</th>\n",
       "      <th>love</th>\n",
       "      <th>man</th>\n",
       "      <th>meet</th>\n",
       "      <th>nice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hey love Bangladesh</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Good afternoon happy</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>live Germany</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nice meet man</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iPhone</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      afternoon  bangladesh  germany  good  happy  hey  \\\n",
       "text                                                                     \n",
       "Hey love Bangladesh           0           1        0     0      0    1   \n",
       "Good afternoon happy          1           0        0     1      1    0   \n",
       "live Germany                  0           0        1     0      0    0   \n",
       "Nice meet man                 0           0        0     0      0    0   \n",
       "iPhone                        0           0        0     0      0    0   \n",
       "\n",
       "                      iphone  live  love  man  meet  nice  \n",
       "text                                                       \n",
       "Hey love Bangladesh        0     0     1    0     0     0  \n",
       "Good afternoon happy       0     0     0    0     0     0  \n",
       "live Germany               0     1     0    0     0     0  \n",
       "Nice meet man              0     0     0    1     1     1  \n",
       "iPhone                     1     0     0    0     0     0  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed5e490",
   "metadata": {},
   "source": [
    "## TF-IDFVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fcdd9f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv_x=tfv.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "83552a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 12 stored elements and shape (5, 12)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6622ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv_df=pd.DataFrame(tfv_x.toarray(),index=df['text'],columns=tfv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fadf6a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>afternoon</th>\n",
       "      <th>bangladesh</th>\n",
       "      <th>germany</th>\n",
       "      <th>good</th>\n",
       "      <th>happy</th>\n",
       "      <th>hey</th>\n",
       "      <th>iphone</th>\n",
       "      <th>live</th>\n",
       "      <th>love</th>\n",
       "      <th>man</th>\n",
       "      <th>meet</th>\n",
       "      <th>nice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hey love Bangladesh</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Good afternoon happy</th>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>live Germany</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nice meet man</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iPhone</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      afternoon  bangladesh   germany     good    happy  \\\n",
       "text                                                                      \n",
       "Hey love Bangladesh     0.00000     0.57735  0.000000  0.00000  0.00000   \n",
       "Good afternoon happy    0.57735     0.00000  0.000000  0.57735  0.57735   \n",
       "live Germany            0.00000     0.00000  0.707107  0.00000  0.00000   \n",
       "Nice meet man           0.00000     0.00000  0.000000  0.00000  0.00000   \n",
       "iPhone                  0.00000     0.00000  0.000000  0.00000  0.00000   \n",
       "\n",
       "                          hey  iphone      live     love      man     meet  \\\n",
       "text                                                                         \n",
       "Hey love Bangladesh   0.57735     0.0  0.000000  0.57735  0.00000  0.00000   \n",
       "Good afternoon happy  0.00000     0.0  0.000000  0.00000  0.00000  0.00000   \n",
       "live Germany          0.00000     0.0  0.707107  0.00000  0.00000  0.00000   \n",
       "Nice meet man         0.00000     0.0  0.000000  0.00000  0.57735  0.57735   \n",
       "iPhone                0.00000     1.0  0.000000  0.00000  0.00000  0.00000   \n",
       "\n",
       "                         nice  \n",
       "text                           \n",
       "Hey love Bangladesh   0.00000  \n",
       "Good afternoon happy  0.00000  \n",
       "live Germany          0.00000  \n",
       "Nice meet man         0.57735  \n",
       "iPhone                0.00000  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e39a76d",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "106ac8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "05275170",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "classifier=pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998749494552612}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('The course from aiQuest is amazing!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9de5a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save_pretrained('sentiment_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165201a",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1aef8f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e358e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6db842dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector= [TreebankWordTokenizer().tokenize(test) for test in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dad45cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hey', 'love', 'Bangladesh'],\n",
       " ['Good', 'afternoon', 'happy'],\n",
       " ['live', 'Germany'],\n",
       " ['Nice', 'meet', 'man'],\n",
       " ['iPhone']]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9bc9f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Word2Vec(text_vector,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "365ff6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x20eead15390>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f45db8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bangladesh', 0.21617142856121063),\n",
       " ('afternoon', 0.09291722625494003),\n",
       " ('Hey', 0.07963486760854721),\n",
       " ('love', 0.06285078823566437),\n",
       " ('Good', 0.0270574688911438),\n",
       " ('happy', 0.016134677454829216),\n",
       " ('man', -0.01083916611969471),\n",
       " ('Germany', -0.027750369161367416),\n",
       " ('meet', -0.05234673246741295),\n",
       " ('live', -0.059876296669244766)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('iPhone')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv AiQuest)",
   "language": "python",
   "name": "aiquest-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
